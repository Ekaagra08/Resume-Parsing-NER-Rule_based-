{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom-Resume-NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "126-XSs3AAzZO9SNwYbqgXQA4L5wYsfHY",
      "authorship_tag": "ABX9TyMz1Da5AzbE7NHp2TVDk+vu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekaagra08/Resume-Parsing-NER-Rule_based-/blob/main/Custom_Resume_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmgHhmjeOhIF",
        "outputId": "01276c84-92ad-455d-f18a-337def5863b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy==2.1.4 in /usr/local/lib/python3.7/dist-packages (2.1.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (0.9.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (0.9.6)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.0.6)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (21.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (57.4.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (0.18.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.4) (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ndf-hboQ0WVn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import random\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ36dM4S1GID",
        "outputId": "05a02913-6de7-431b-cc3b-8666d80cbb31"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4 MB 1.2 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_json_for_spacy(FilePath):\n",
        "    try:\n",
        "      converted_data = []\n",
        "      lines=[]\n",
        "      with open(FilePath, 'r') as f:\n",
        "          lines = f.readlines()\n",
        "\n",
        "      for line in lines:\n",
        "          data = json.loads(line)\n",
        "          text = data['content'].replace(\"\\n\", \" \")\n",
        "          entities = []\n",
        "          data_annotations = data['annotation']\n",
        "          if data_annotations is not None:\n",
        "              for annotation in data_annotations:\n",
        "                  #only a single point in text annotation.\n",
        "                  point = annotation['points'][0]\n",
        "                  labels = annotation['label']\n",
        "                  # handle both list of labels or a single label.\n",
        "                  if not isinstance(labels, list):\n",
        "                      labels = [labels]\n",
        "\n",
        "                  for label in labels:\n",
        "                      point_start = point['start']\n",
        "                      point_end = point['end']\n",
        "                      point_text = point['text']\n",
        "\n",
        "                      lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                      rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                      if lstrip_diff != 0:\n",
        "                          point_start = point_start + lstrip_diff\n",
        "                      if rstrip_diff != 0:\n",
        "                          point_end = point_end - rstrip_diff\n",
        "                      entities.append((point_start, point_end + 1 , label))\n",
        "          \n",
        "          converted_data.append((text, {\"entities\" : entities}))\n",
        "      return converted_data\n",
        "      \n",
        "    except Exception as e:\n",
        "      logging.exception(\"Unable to process \" + FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "      return None    \n",
        "\n",
        "################################################################################################\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "# removes extra white spaces from entity span to prevent overlaping\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    \n",
        "    return cleaned_data\n"
      ],
      "metadata": {
        "id": "qVWRtkfMC7ce"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_clean = trim_entity_spans(convert_json_for_spacy(\"/content/drive/MyDrive/Resume Parsing (NER + Rule-based)/traindata.json\"))\n",
        "print(train_data_clean[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UEZQKE4KIzA",
        "outputId": "71aff3d7-95de-48bf-dc24-d9707b5bd6f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/', {'entities': [[1749, 1755, 'Companies worked at'], [1696, 1702, 'Companies worked at'], [1417, 1423, 'Companies worked at'], [1356, 1793, 'Skills'], [1209, 1215, 'Companies worked at'], [1136, 1247, 'Skills'], [928, 932, 'Graduation Year'], [858, 889, 'College Name'], [821, 856, 'Degree'], [787, 791, 'Graduation Year'], [744, 750, 'Companies worked at'], [722, 742, 'Designation'], [658, 664, 'Companies worked at'], [640, 656, 'Designation'], [574, 580, 'Companies worked at'], [555, 572, 'Designation'], [470, 493, 'Companies worked at'], [444, 468, 'Designation'], [308, 314, 'Companies worked at'], [234, 240, 'Companies worked at'], [175, 198, 'Companies worked at'], [93, 136, 'Email Address'], [39, 48, 'Location'], [13, 37, 'Designation'], [0, 12, 'Name']]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_spacyNER():\n",
        "\n",
        "  # creating blank eng-language class and add the built-in pipeline components to the pipeline\n",
        "  c_nlp = spacy.blank(\"en\")\n",
        "  if 'ner' not in c_nlp.pipe_names:\n",
        "    ner = c_nlp.create_pipe('ner')\n",
        "    c_nlp.add_pipe(ner,last = True)\n",
        "\n",
        "  # adding custom lables from resume\n",
        "  for _, annotation in train_data_clean:\n",
        "    for ent in annotation.get('entities'):\n",
        "      ner.add_label(ent[2])    \n",
        "\n",
        "  # other pipes to disabled during training\n",
        "  other_pipes = [pipe for pipe in c_nlp.pipe_names if pipe != 'ner']\n",
        "  with c_nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "    optimizer = c_nlp.begin_training()\n",
        "    for itn in range(10):\n",
        "      print(\"Statring iteration \" + str(itn + 1))\n",
        "      random.shuffle(train_data_clean)\n",
        "      losses = {}\n",
        "      for text, annotations in train_data_clean:\n",
        "        c_nlp.update(\n",
        "            [text],  # batch of texts\n",
        "            [annotations],  # batch of annotations\n",
        "            drop=0.2,  # dropout - make it harder to memorise data\n",
        "            sgd=optimizer,  # callable to update weights\n",
        "            losses=losses)\n",
        "      print(losses)\n",
        "  return c_nlp   "
      ],
      "metadata": {
        "id": "nUQL7e93Km32"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = train_spacyNER()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Meashv-zLnpI",
        "outputId": "7dd67c1f-a1ec-47eb-d70c-ac2fbede4fa5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statring iteration 1\n",
            "{'ner': 31139.00876891847}\n",
            "Statring iteration 2\n",
            "{'ner': 23121.033560894088}\n",
            "Statring iteration 3\n",
            "{'ner': 16716.233918247923}\n",
            "Statring iteration 4\n",
            "{'ner': 13632.157929476387}\n",
            "Statring iteration 5\n",
            "{'ner': 13088.061687318463}\n",
            "Statring iteration 6\n",
            "{'ner': 10983.932519705359}\n",
            "Statring iteration 7\n",
            "{'ner': 10801.565972050092}\n",
            "Statring iteration 8\n",
            "{'ner': 9212.50325682725}\n",
            "Statring iteration 9\n",
            "{'ner': 10858.665097953852}\n",
            "Statring iteration 10\n",
            "{'ner': 9099.141343083038}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(train_data_clean[0][0])\n",
        "for ent in doc.ents:\n",
        "  print(f'{ent.label_.upper():{25}}- {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqQMQuztVfIo",
        "outputId": "781ffd6f-ffa3-4c13-bd35-764589f252d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                     - Manisha Bharti\n",
            "DESIGNATION              - Software Automation Engineer\n",
            "LOCATION                 - Pune\n",
            "EMAIL ADDRESS            - indeed.com/r/Manisha-Bharti/3573e36088ddc073\n",
            "YEARS OF EXPERIENCE      - 3.5 years\n",
            "COMPANIES WORKED AT      - Infosys Limited\n",
            "LOCATION                 - Pune\n",
            "DESIGNATION              - NOT WORKING\n",
            "DESIGNATION              - Software Automation Engineer\n",
            "COMPANIES WORKED AT      - Infosys Limited\n",
            "DESIGNATION              - System Engineer Trainee\n",
            "COMPANIES WORKED AT      - Infosys Limited\n",
            "DEGREE                   - B.Tech in CSE\n",
            "COLLEGE NAME             - Meghnad saha institute of technology\n",
            "GRADUATION YEAR          - 2013\n",
            "SKILLS                   - Uft/qtp,alm/qc,jira,jenkins,automation testing,cicd,service vitualization,uipath    ADDITIONAL INFORMATION  Operating Systems Windows 10 / 8 / 7 / Vista / XP  Domains Banking and Finance\n",
            "COMPANIES WORKED AT      - Oracle\n",
            "COMPANIES WORKED AT      - Oracle\n",
            "SKILLS                   - Has\n",
            "SKILLS                   - Has\n",
            "SKILLS                   - Has\n",
            "COMPANIES WORKED AT      - Infosys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model and evaluate it\n",
        "examples = convert_json_for_spacy(\"/content/drive/MyDrive/Resume Parsing (NER + Rule-based)/testdata.json\")\n",
        "tp=0\n",
        "tr=0\n",
        "tf=0\n",
        "ta=0\n",
        "c=0        \n",
        "for text,annot in examples:\n",
        "\n",
        "    f=open(\"resume\"+str(c)+\".txt\",\"w\")\n",
        "    doc_to_test=nlp(text)\n",
        "    d={}\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_]=[]\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_].append(ent.text)\n",
        "\n",
        "    for i in set(d.keys()):\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(i +\":\"+\"\\n\")\n",
        "        for j in set(d[i]):\n",
        "            f.write(j.replace('\\n','')+\"\\n\")\n",
        "    d={}\n",
        "    for ent in doc_to_test.ents:\n",
        "        d[ent.label_]=[0,0,0,0,0,0]\n",
        "    for ent in doc_to_test.ents:\n",
        "        doc_gold_text= nlp.make_doc(text)\n",
        "        gold = GoldParse(doc_gold_text, entities=annot.get(\"entities\"))\n",
        "        y_true = [ent.label_ if ent.label_ in x else 'Not '+ent.label_ for x in gold.ner]\n",
        "        y_pred = [x.ent_type_ if x.ent_type_ ==ent.label_ else 'Not '+ent.label_ for x in doc_to_test]  \n",
        "        if(d[ent.label_][0]==0):\n",
        "            #f.write(\"For Entity \"+ent.label_+\"\\n\")   \n",
        "            #f.write(classification_report(y_true, y_pred)+\"\\n\")\n",
        "            (p,r,f,s)= precision_recall_fscore_support(y_true,y_pred,average='weighted')\n",
        "            a=accuracy_score(y_true,y_pred)\n",
        "            d[ent.label_][0]=1\n",
        "            d[ent.label_][1]+=p\n",
        "            d[ent.label_][2]+=r\n",
        "            d[ent.label_][3]+=f\n",
        "            d[ent.label_][4]+=a\n",
        "            d[ent.label_][5]+=1\n",
        "    c+=1\n",
        "for i in d:\n",
        "    print(\"\\n For Entity \"+i+\"\\n\")\n",
        "    print(\"Accuracy : \"+str((d[i][4]/d[i][5])*100)+\"%\")\n",
        "    print(\"Precision : \"+str(d[i][1]/d[i][5]))\n",
        "    print(\"Recall : \"+str(d[i][2]/d[i][5]))\n",
        "    print(\"F-score : \"+str(d[i][3]/d[i][5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XcFPWkIZxtj",
        "outputId": "e457cfff-8712-4498-ffe3-e174affd6a75"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " For Entity Name\n",
            "\n",
            "Accuracy : 99.82683982683983%\n",
            "Precision : 0.9982714019140732\n",
            "Recall : 0.9982683982683983\n",
            "F-score : 0.9979805495430495\n",
            "\n",
            " For Entity Location\n",
            "\n",
            "Accuracy : 99.3073593073593%\n",
            "Precision : 0.9931216097593915\n",
            "Recall : 0.9930735930735931\n",
            "F-score : 0.9903150762281195\n",
            "\n",
            " For Entity Email Address\n",
            "\n",
            "Accuracy : 99.48051948051948%\n",
            "Precision : 1.0\n",
            "Recall : 0.9948051948051948\n",
            "F-score : 0.9973958333333334\n",
            "\n",
            " For Entity Designation\n",
            "\n",
            "Accuracy : 100.0%\n",
            "Precision : 1.0\n",
            "Recall : 1.0\n",
            "F-score : 1.0\n",
            "\n",
            " For Entity Companies worked at\n",
            "\n",
            "Accuracy : 99.13419913419914%\n",
            "Precision : 0.9914170174135513\n",
            "Recall : 0.9913419913419913\n",
            "F-score : 0.9877533258734302\n",
            "\n",
            " For Entity College Name\n",
            "\n",
            "Accuracy : 100.0%\n",
            "Precision : 1.0\n",
            "Recall : 1.0\n",
            "F-score : 1.0\n",
            "\n",
            " For Entity Graduation Year\n",
            "\n",
            "Accuracy : 100.0%\n",
            "Precision : 1.0\n",
            "Recall : 1.0\n",
            "F-score : 1.0\n",
            "\n",
            " For Entity Skills\n",
            "\n",
            "Accuracy : 100.0%\n",
            "Precision : 1.0\n",
            "Recall : 1.0\n",
            "F-score : 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "filename = 'NER_model.pkl'\n",
        "pickle.dump(nlp, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "NnofzHqZsazE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "Ok21TIavfiQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae5cf37-0b1c-41c0-c5c8-0f453e0e9fd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 25.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.19.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, fitz\n",
        "\n",
        "def parse(filepath):\n",
        "  try:\n",
        "    doc = fitz.open(filepath)\n",
        "    textblob = \"\"\n",
        "    for pg in doc:\n",
        "      textblob += str(pg.get_text())\n",
        "\n",
        "  except:\n",
        "    print(\"\\nError in parse: Could not read the file \")\n",
        "\n",
        "  finally:\n",
        "    doc.close()\n",
        "    \n",
        "  return(textblob)  "
      ],
      "metadata": {
        "id": "zZYlqdBVqqmC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"/content/drive/MyDrive/Resume/Ekaagra Dubey Resume.pdf\"\n",
        "text = parse(filepath)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "ePJH0IO42GJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = NER_model(text)\n",
        "def present\n",
        "for ent in doc.ents:\n",
        "  print(f'{ent.label_.upper():{25}}- {ent.text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ74b86cvqq4",
        "outputId": "d32f1d94-e0c1-4123-e559-a03bb6b598d9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                     - EKAAGRA DUBEY\n",
            "LOCATION                 - ekaagra@gmail.com |\n",
            "LOCATION                 - Delhi\n",
            "LOCATION                 - Delhi\n",
            "SKILLS                   - Duke University\n",
            "-\n",
            "AWS Machine Learning - Amazon Web Services\n",
            "-\n",
            "Python Programming & Data Exploration\n",
            "LOCATION                 - NIIT\n",
            "COLLEGE NAME             - Hackathons.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class resparser():\n",
        "  def __init__(self,model,filepath):\n",
        "    self.model = model\n",
        "    self.filepath = filepath\n",
        "\n",
        "  def parse(self):\n",
        "\n",
        "    ''' Parses the PDF and displayes resume lables and entities '''\n",
        " \n",
        "    try:\n",
        "      doc = fitz.open(self.filepath)\n",
        "      self.textblob = \"\"\n",
        "      for pg in doc:\n",
        "        self.textblob += str(pg.get_text())\n",
        "\n",
        "    except:\n",
        "      print(\"\\nError in parse: Could not read the file \")\n",
        "\n",
        "    finally:\n",
        "      doc.close()\n",
        "    \n",
        "    nlp = self.model\n",
        "    doc = nlp(self.textblob)\n",
        "    COLOR = '\\033[92m' #GREEN\n",
        "    BOLD = '\\033[1m'\n",
        "    RESET = '\\033[0m' #RESET COLOR\n",
        "\n",
        "    for ent in doc.ents:\n",
        "      print(f\"{COLOR}{BOLD}{ent.label_.upper():{25}}{RESET}: {ent.text}\")\n",
        "    \n",
        "  def text(self):\n",
        "    ''' returns: Text format of parsed PDF resume '''\n",
        "    return self.textblob\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-48kygLbwYKv"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NER_model = pickle.load(open(\"/content/NER_model.pkl\", 'rb'))"
      ],
      "metadata": {
        "id": "dh_V_JFurdIQ"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r3 = resparser(model=NER_model, filepath=\"/content/drive/MyDrive/TPO resume.pdf\")"
      ],
      "metadata": {
        "id": "7fm1eeYaF31c"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r3.parse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnCTbmjaHtzg",
        "outputId": "a4758363-bf4c-4e3c-9a28-bc56a566977d"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95m\u001b[1mNAME                     \u001b[0m: EKAAGRA DUBEY\n",
            "\u001b[95m\u001b[1mSKILLS                   \u001b[0m: CSS\n",
            "\u001b[95m\u001b[1mSKILLS                   \u001b[0m: PHP\n",
            "\u001b[95m\u001b[1mSKILLS                   \u001b[0m: Duke University (Coursera), Online \n",
            "Jun 2020 - Jun 2020 \n",
            " \n",
            "• \n",
            "AWS Machine Learning \n",
            "Amazon Web Services, Online Feb \n",
            "2020 - Mar 2020 \n",
            " \n",
            "• \n",
            "Python Programming And Data Exploration \n",
            "NIIT, Delhi \n",
            "Aug 2019 - Nov 2019 \n",
            " \n",
            "SOFT SKILLS \n",
            "• \n",
            "Problem solving \n",
            "• \n",
            "Work ethics \n",
            "• \n",
            "Leadership \n",
            "• \n",
            "Adaptability \n",
            "• \n",
            "Time management \n",
            "• \n",
            "Interpersonal communication \n",
            " \n",
            "EXTRA-CURRICULAR ACTIVITIES  \n",
            "• \n",
            "Participated in the Goldman Sachs Engineering Virtual program with Forage. certificate link \n",
            "• \n",
            "Participation in multiple Data science and machine learning hackathons \n",
            "• \n",
            "Football  \n",
            "• \n",
            "Drawing and art \n",
            "  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(r3.text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfeQ7OKqH1Xs",
        "outputId": "35d15697-a552-48ac-cf15-8bff5b17a651"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EKAAGRA DUBEY    \n",
            "Email: ekaagra@gmail.com  \n",
            "Linkedin: https://www.linkedin.com/in/ekaagra-dubey-e08/   \n",
            "Github:  https://github.com/Ekaagra08  \n",
            "Phone No. : 9958582804, 7982035804  \n",
            "Greater Noida West, Uttar Pradesh, India 201306 \n",
            " \n",
            " \n",
            "ABOUT  \n",
            "A data science enthusiast. Practice in executing full end to end data science projects. Ramping up \n",
            "projects within time, budget and quality parameters, as per project management and best practice \n",
            "guidelines, targeting assignments in Data Science, Data analytics, Machine Learning and Deep \n",
            "learning.  \n",
            "  \n",
            "EDUCATION  \n",
            "• \n",
            "Bachelor of Technology (B.Tech), Information Technology \n",
            "Passing year: 2022 \n",
            "ADGITM college (Guru Gobind Singh Indraprastha University)  \n",
            "CGPA: 8.8/10  \n",
            " \n",
            "• \n",
            "Senior Secondary (XII), Science  \n",
            "Cambridge School Srinivaspuri (CBSE board)  \n",
            "Passing year: 2018  \n",
            "Percentage: 84.00%  \n",
            " \n",
            "SKILLS  \n",
            "Python  \n",
            "Operating system \n",
            "Keras, tensorflow \n",
            "C++ \n",
            "Problem solving \n",
            "Web Scraping  \n",
            "Java \n",
            "Machine learning  \n",
            "NLP \n",
            "Data Analytics  \n",
            "Deep learning  \n",
            "Flask  \n",
            "RDBMS \n",
            "Computer vision  \n",
            "HTML, CSS, Bootstrap  \n",
            "AWS \n",
            "Image processing  \n",
            "javascript  \n",
            "Data structures and algorithms \n",
            "OpenCV   \n",
            "PHP, AJAX  \n",
            " \n",
            "PROJECTS/EXPERIENCE \n",
            "• \n",
            "Real-time object detection (Jul 2021 - Aug 2021) \n",
            "Detecting object and classifying in real time using YOLOv3 and OpenCV.  \n",
            " \n",
            "• \n",
            "Image classification (Jul 2021 - Jul 2021) \n",
            "Classifying images using pre trained deep neural network and OpenCV library.  \n",
            " \n",
            "• \n",
            "Tweets sentiment analysis (Mar 2021 - Apr 2021) \n",
            "Scraped tweets using python, Twitter API and did sentiment analysis on them.  \n",
            " \n",
            "• \n",
            "Spam sms classification (Jan 2021 - Feb 2021) \n",
            "Sms classification done using pandas, sklearn, NLTK and deployed using flask.  \n",
            " \n",
            "• \n",
            "Real-estate price prediction (Dec 2020 - Jan 2021)  \n",
            "Predicting Delhi house prices  based on different features and plot the location of house in a similar price range on \n",
            "map. An end-to-end machine learning project made using python, numpy, pandas, sklearn and deployed using flask.  \n",
            "  \n",
            " \n",
            "TRAININGS/CERTIFICATIONS \n",
            "• \n",
            "Deep Learning \n",
            "Deeplearning.ai, Coursera, Online \n",
            "May 2021 – Present \n",
            " \n",
            "• \n",
            "Machine Learning \n",
            "Stanford University, Coursera, Online Dec \n",
            "2020 - Feb 2021 \n",
            " \n",
            "• \n",
            "DATA SCIENCE PROFESSIONAL \n",
            "IBM, Coursera \n",
            "Sep 2020 - Nov 2020 \n",
            " \n",
            "• \n",
            "Data Science Math Skills \n",
            "Duke University (Coursera), Online \n",
            "Jun 2020 - Jun 2020 \n",
            " \n",
            "• \n",
            "AWS Machine Learning \n",
            "Amazon Web Services, Online Feb \n",
            "2020 - Mar 2020 \n",
            " \n",
            "• \n",
            "Python Programming And Data Exploration \n",
            "NIIT, Delhi \n",
            "Aug 2019 - Nov 2019 \n",
            " \n",
            "SOFT SKILLS \n",
            "• \n",
            "Problem solving \n",
            "• \n",
            "Work ethics \n",
            "• \n",
            "Leadership \n",
            "• \n",
            "Adaptability \n",
            "• \n",
            "Time management \n",
            "• \n",
            "Interpersonal communication \n",
            " \n",
            "EXTRA-CURRICULAR ACTIVITIES  \n",
            "• \n",
            "Participated in the Goldman Sachs Engineering Virtual program with Forage. certificate link \n",
            "• \n",
            "Participation in multiple Data science and machine learning hackathons \n",
            "• \n",
            "Football  \n",
            "• \n",
            "Drawing and art \n",
            "  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ixUKoKsfSKO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}